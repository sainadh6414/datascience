{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:04:40.887320Z",
     "start_time": "2021-06-25T05:04:32.169626Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1267db57c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;31m# import spacy library. check first time installation instructions on spacy website https://spacy.io/usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy # import spacy library. check first time installation instructions on spacy website https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:05:12.033223Z",
     "start_time": "2021-06-25T05:05:11.259147Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') #load spacy model and save it with a variable name, here calling it nlp. 'en' stnads for english languate. spacy supports multiple languages. So we need to specifically load the english language model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:05:15.605253Z",
     "start_time": "2021-06-25T05:05:15.588571Z"
    }
   },
   "outputs": [],
   "source": [
    "# some random text\n",
    "text = 'The cutting edge technologies available today can make life enormously exiting as well as dangerous and complicated. This can also be a mischievous comment generating controversies across the political spectrum. This can spill over beyond India and impact the rest of the world to the tune of $ 10 million'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spacy Document Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:05:35.491156Z",
     "start_time": "2021-06-25T05:05:35.405385Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text) # creating a spacy document object. This stores a lot of info about each word(token) in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:05:57.299371Z",
     "start_time": "2021-06-25T05:05:57.285412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The cutting edge technologies available today can make life enormously exiting as well as dangerous and complicated. This can also be a mischievous comment generating controversies across the political spectrum. This can spill over beyond India and impact the rest of the world to the tune of $ 10 million"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc # displays the text content as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Sentences from a Spacy Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:06:26.480904Z",
     "start_time": "2021-06-25T05:06:26.462953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cutting edge technologies available today can make life enormously exiting as well as dangerous and complicated.\n",
      "This can also be a mischievous comment generating controversies across the political spectrum.\n",
      "This can spill over beyond India and impact the rest of the world to the tune of $ 10 million\n"
     ]
    }
   ],
   "source": [
    "# extracting sentences from the spacy doc\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Part of Speech Tags from a Spacy Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:10:15.920825Z",
     "start_time": "2021-06-25T05:10:15.898885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 DET       determiner\n",
      "cutting             VERB      verb\n",
      "edge                NOUN      noun\n",
      "technologies        NOUN      noun\n",
      "available           ADJ       adjective\n",
      "today               NOUN      noun\n",
      "can                 AUX       auxiliary\n",
      "make                VERB      verb\n",
      "life                NOUN      noun\n",
      "enormously          ADV       adverb\n",
      "exiting             VERB      verb\n",
      "as                  ADV       adverb\n",
      "well                ADV       adverb\n",
      "as                  ADP       adposition\n",
      "dangerous           ADJ       adjective\n",
      "and                 CCONJ     coordinating conjunction\n",
      "complicated         ADJ       adjective\n",
      ".                   PUNCT     punctuation\n",
      "This                DET       determiner\n",
      "can                 AUX       auxiliary\n",
      "also                ADV       adverb\n",
      "be                  VERB      verb\n",
      "a                   DET       determiner\n",
      "mischievous         ADJ       adjective\n",
      "comment             NOUN      noun\n",
      "generating          NOUN      noun\n",
      "controversies       NOUN      noun\n",
      "across              ADP       adposition\n",
      "the                 DET       determiner\n",
      "political           ADJ       adjective\n",
      "spectrum            NOUN      noun\n",
      ".                   PUNCT     punctuation\n",
      "This                DET       determiner\n",
      "can                 AUX       auxiliary\n",
      "spill               VERB      verb\n",
      "over                ADP       adposition\n",
      "beyond              ADP       adposition\n",
      "India               PROPN     proper noun\n",
      "and                 CCONJ     coordinating conjunction\n",
      "impact              VERB      verb\n",
      "the                 DET       determiner\n",
      "rest                NOUN      noun\n",
      "of                  ADP       adposition\n",
      "the                 DET       determiner\n",
      "world               NOUN      noun\n",
      "to                  ADP       adposition\n",
      "the                 DET       determiner\n",
      "tune                NOUN      noun\n",
      "of                  ADP       adposition\n",
      "$                   SYM       symbol\n",
      "10                  NUM       numeral\n",
      "million             NUM       numeral\n"
     ]
    }
   ],
   "source": [
    "# part of speech (pos) tags --> vrb, noun, adjective etc.\n",
    "for token in doc:\n",
    "    print(f'{token.text:20}{token.pos_:10}{spacy.explain(token.pos_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized Form of Words using `lemma_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:12:59.542265Z",
     "start_time": "2021-06-25T05:12:59.532290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 the\n",
      "cutting             cut\n",
      "edge                edge\n",
      "technologies        technology\n",
      "available           available\n",
      "today               today\n",
      "can                 can\n",
      "make                make\n",
      "life                life\n",
      "enormously          enormously\n",
      "exiting             exit\n",
      "as                  as\n",
      "well                well\n",
      "as                  as\n",
      "dangerous           dangerous\n",
      "and                 and\n",
      "complicated         complicated\n",
      ".                   .\n",
      "This                this\n",
      "can                 can\n",
      "also                also\n",
      "be                  be\n",
      "a                   a\n",
      "mischievous         mischievous\n",
      "comment             comment\n",
      "generating          generating\n",
      "controversies       controversy\n",
      "across              across\n",
      "the                 the\n",
      "political           political\n",
      "spectrum            spectrum\n",
      ".                   .\n",
      "This                this\n",
      "can                 can\n",
      "spill               spill\n",
      "over                over\n",
      "beyond              beyond\n",
      "India               India\n",
      "and                 and\n",
      "impact              impact\n",
      "the                 the\n",
      "rest                rest\n",
      "of                  of\n",
      "the                 the\n",
      "world               world\n",
      "to                  to\n",
      "the                 the\n",
      "tune                tune\n",
      "of                  of\n",
      "$                   $\n",
      "10                  10\n",
      "million             million\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing is a smarter way of getting the root words compared to stemming\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:20}{token.lemma_}') # using .lemma_ we can access the lemmatized form of each token\n",
    "\n",
    "# token.text prints the string form of the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:59:18.854349Z",
     "start_time": "2021-06-25T05:59:18.843374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today               DATE\n",
      "India               GPE\n",
      "$ 10 million        MONEY\n"
     ]
    }
   ],
   "source": [
    "# named entity recognition (NER) --> Name, place, person, location, country, city, state, continent,Money etc.\n",
    "\n",
    "for entities in doc.ents:\n",
    "    print(f'{entities.text:20}{entities.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is smart enough to combine $ and 10 and million and understand that these terms together means money !!\n",
    "GPE stands for geographic or polical Entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:14:33.831506Z",
     "start_time": "2021-06-25T05:14:31.242193Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:14:35.649588Z",
     "start_time": "2021-06-25T05:14:35.631636Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:15:27.035889Z",
     "start_time": "2021-06-25T05:15:27.024917Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:16:07.634639Z",
     "start_time": "2021-06-25T05:16:07.624666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 the\n",
      "cutting             cut\n",
      "edge                edg\n",
      "technologies        technolog\n",
      "available           avail\n",
      "today               today\n",
      "can                 can\n",
      "make                make\n",
      "life                life\n",
      "enormously          enorm\n",
      "exiting             exit\n",
      "as                  as\n",
      "well                well\n",
      "as                  as\n",
      "dangerous           danger\n",
      "and                 and\n",
      "complicated         complic\n",
      ".                   .\n",
      "This                thi\n",
      "can                 can\n",
      "also                also\n",
      "be                  be\n",
      "a                   a\n",
      "mischievous         mischiev\n",
      "comment             comment\n",
      "generating          gener\n",
      "controversies       controversi\n",
      "across              across\n",
      "the                 the\n",
      "political           polit\n",
      "spectrum            spectrum\n",
      ".                   .\n",
      "This                thi\n",
      "can                 can\n",
      "spill               spill\n",
      "over                over\n",
      "beyond              beyond\n",
      "India               india\n",
      "and                 and\n",
      "impact              impact\n",
      "the                 the\n",
      "rest                rest\n",
      "of                  of\n",
      "the                 the\n",
      "world               world\n",
      "to                  to\n",
      "the                 the\n",
      "tune                tune\n",
      "of                  of\n",
      "$                   $\n",
      "10                  10\n",
      "million             million\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:20}{stemmer.stem(token.text)}') # compare the stemmed form using nltk with the lemmatized form using spacy earlier... not so good ! many stemmed form of the words have no meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:24:08.255207Z",
     "start_time": "2021-06-25T05:24:08.243238Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:29:51.636345Z",
     "start_time": "2021-06-25T05:29:51.625407Z"
    }
   },
   "outputs": [],
   "source": [
    "lems = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:32:05.069879Z",
     "start_time": "2021-06-25T05:32:05.059906Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize # As WordNetLemmatizer works only with nltk tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:32:46.391864Z",
     "start_time": "2021-06-25T05:32:46.359950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'cutting',\n",
       " 'edge',\n",
       " 'technologies',\n",
       " 'available',\n",
       " 'today',\n",
       " 'can',\n",
       " 'make',\n",
       " 'life',\n",
       " 'enormously',\n",
       " 'exiting',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'dangerous',\n",
       " 'and',\n",
       " 'complicated',\n",
       " '.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'also',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mischievous',\n",
       " 'comment',\n",
       " 'generating',\n",
       " 'controversies',\n",
       " 'across',\n",
       " 'the',\n",
       " 'political',\n",
       " 'spectrum',\n",
       " '.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'spill',\n",
       " 'over',\n",
       " 'beyond',\n",
       " 'India',\n",
       " 'and',\n",
       " 'impact',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'the',\n",
       " 'tune',\n",
       " 'of',\n",
       " '$',\n",
       " '10',\n",
       " 'million']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = word_tokenize(text)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:35:42.743859Z",
     "start_time": "2021-06-25T05:35:42.723912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 The\n",
      "cutting             cutting\n",
      "edge                edge\n",
      "technologies        technology\n",
      "available           available\n",
      "today               today\n",
      "can                 can\n",
      "make                make\n",
      "life                life\n",
      "enormously          enormously\n",
      "exiting             exiting\n",
      "as                  a\n",
      "well                well\n",
      "as                  a\n",
      "dangerous           dangerous\n",
      "and                 and\n",
      "complicated         complicated\n",
      ".                   .\n",
      "This                This\n",
      "can                 can\n",
      "also                also\n",
      "be                  be\n",
      "a                   a\n",
      "mischievous         mischievous\n",
      "comment             comment\n",
      "generating          generating\n",
      "controversies       controversy\n",
      "across              across\n",
      "the                 the\n",
      "political           political\n",
      "spectrum            spectrum\n",
      ".                   .\n",
      "This                This\n",
      "can                 can\n",
      "spill               spill\n",
      "over                over\n",
      "beyond              beyond\n",
      "India               India\n",
      "and                 and\n",
      "impact              impact\n",
      "the                 the\n",
      "rest                rest\n",
      "of                  of\n",
      "the                 the\n",
      "world               world\n",
      "to                  to\n",
      "the                 the\n",
      "tune                tune\n",
      "of                  of\n",
      "$                   $\n",
      "10                  10\n",
      "million             million\n"
     ]
    }
   ],
   "source": [
    "for token in toks:\n",
    "    print(f'{token:20}{lems.lemmatize(token)}') # ouput much better than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:44:58.447091Z",
     "start_time": "2021-06-25T05:44:58.431134Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe for comparison of stemmed and lemmatized form of the words\n",
    "stemmed = []\n",
    "lemmatized = []\n",
    "\n",
    "for token in toks:\n",
    "    stemmed.append(stemmer.stem(token))\n",
    "    lemmatized.append(lems.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:45:22.582311Z",
     "start_time": "2021-06-25T05:45:21.732536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T05:46:51.290743Z",
     "start_time": "2021-06-25T05:46:51.260823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cutting</td>\n",
       "      <td>cut</td>\n",
       "      <td>cutting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edge</td>\n",
       "      <td>edg</td>\n",
       "      <td>edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technologies</td>\n",
       "      <td>technolog</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>available</td>\n",
       "      <td>avail</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>today</td>\n",
       "      <td>today</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>make</td>\n",
       "      <td>make</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enormously</td>\n",
       "      <td>enorm</td>\n",
       "      <td>enormously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>exiting</td>\n",
       "      <td>exit</td>\n",
       "      <td>exiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>well</td>\n",
       "      <td>well</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dangerous</td>\n",
       "      <td>danger</td>\n",
       "      <td>dangerous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>complicated</td>\n",
       "      <td>complic</td>\n",
       "      <td>complicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This</td>\n",
       "      <td>thi</td>\n",
       "      <td>This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>also</td>\n",
       "      <td>also</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mischievous</td>\n",
       "      <td>mischiev</td>\n",
       "      <td>mischievous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>generating</td>\n",
       "      <td>gener</td>\n",
       "      <td>generating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>controversies</td>\n",
       "      <td>controversi</td>\n",
       "      <td>controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>across</td>\n",
       "      <td>across</td>\n",
       "      <td>across</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>political</td>\n",
       "      <td>polit</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>spectrum</td>\n",
       "      <td>spectrum</td>\n",
       "      <td>spectrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>This</td>\n",
       "      <td>thi</td>\n",
       "      <td>This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spill</td>\n",
       "      <td>spill</td>\n",
       "      <td>spill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>over</td>\n",
       "      <td>over</td>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>beyond</td>\n",
       "      <td>beyond</td>\n",
       "      <td>beyond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>India</td>\n",
       "      <td>india</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>impact</td>\n",
       "      <td>impact</td>\n",
       "      <td>impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rest</td>\n",
       "      <td>rest</td>\n",
       "      <td>rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tune</td>\n",
       "      <td>tune</td>\n",
       "      <td>tune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         original      stemmed   lemmatized\n",
       "0             The          the          The\n",
       "1         cutting          cut      cutting\n",
       "2            edge          edg         edge\n",
       "3    technologies    technolog   technology\n",
       "4       available        avail    available\n",
       "5           today        today        today\n",
       "6             can          can          can\n",
       "7            make         make         make\n",
       "8            life         life         life\n",
       "9      enormously        enorm   enormously\n",
       "10        exiting         exit      exiting\n",
       "11             as           as            a\n",
       "12           well         well         well\n",
       "13             as           as            a\n",
       "14      dangerous       danger    dangerous\n",
       "15            and          and          and\n",
       "16    complicated      complic  complicated\n",
       "17              .            .            .\n",
       "18           This          thi         This\n",
       "19            can          can          can\n",
       "20           also         also         also\n",
       "21             be           be           be\n",
       "22              a            a            a\n",
       "23    mischievous     mischiev  mischievous\n",
       "24        comment      comment      comment\n",
       "25     generating        gener   generating\n",
       "26  controversies  controversi  controversy\n",
       "27         across       across       across\n",
       "28            the          the          the\n",
       "29      political        polit    political\n",
       "30       spectrum     spectrum     spectrum\n",
       "31              .            .            .\n",
       "32           This          thi         This\n",
       "33            can          can          can\n",
       "34          spill        spill        spill\n",
       "35           over         over         over\n",
       "36         beyond       beyond       beyond\n",
       "37          India        india        India\n",
       "38            and          and          and\n",
       "39         impact       impact       impact\n",
       "40            the          the          the\n",
       "41           rest         rest         rest\n",
       "42             of           of           of\n",
       "43            the          the          the\n",
       "44          world        world        world\n",
       "45             to           to           to\n",
       "46            the          the          the\n",
       "47           tune         tune         tune\n",
       "48             of           of           of\n",
       "49              $            $            $\n",
       "50             10           10           10\n",
       "51        million      million      million"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict(original = toks,stemmed = stemmed, lemmatized = lemmatized))\n",
    "df # output of lemmatization much more sensible.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

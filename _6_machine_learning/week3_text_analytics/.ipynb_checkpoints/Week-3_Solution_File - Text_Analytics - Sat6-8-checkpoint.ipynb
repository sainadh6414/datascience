{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = {font-weight: 'bold';}>We will be trying to understand sentiment of tweets about the company Apple. By using the twitter data we can hope to understand the public perception a bit better.\n",
    "\n",
    "Our challenge is to see if we can correctly classify tweets as being either positive or negative.\n",
    "\n",
    "Problem Statement:\n",
    "•\tCorrectly classify the tweets as being positive or negative.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using: nltk.NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:10:53.648148Z",
     "start_time": "2021-06-26T13:10:49.130321Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing the necessary libraries along with the standard import\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re # this is the regular expression library which helps us search for or extract matching patterns from a given string\n",
    "import nltk # this is the Natural Language Tool Kit which contains a lot of functionalities for text analytics\n",
    "import matplotlib.pyplot as plt\n",
    "import string # this is used for string manipulations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv file available in the working or specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:11:01.267341Z",
     "start_time": "2021-06-26T13:11:01.250385Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Apple_tweets = pd.read_csv(\"Apple_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2e425d9f1eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:11:08.850095Z",
     "start_time": "2021-06-26T13:11:08.823168Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking the first 5 rows of the dataset\n",
    "\n",
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop non-alphanumeric & space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:14:43.289766Z",
     "start_time": "2021-06-26T13:14:43.251868Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "# \\w: Returns a match where the string contains any alphanumeric characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "# \\s: Returns a match where the string contains a white space character.\n",
    "# [^]: Returns a match for any character EXCEPT what is written after it.\n",
    "Apple_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:19:02.298763Z",
     "start_time": "2021-06-26T13:19:02.264822Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting all the words to lower case\n",
    "\n",
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "Apple_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Target Variable to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:24:12.668627Z",
     "start_time": "2021-06-26T13:24:12.651675Z"
    }
   },
   "outputs": [],
   "source": [
    "## We are defining a function to convert the 'Avg' column into a column with two classes which will be treated as the target variable later\n",
    "\n",
    "def get_senti(x): \n",
    "    if x >= 0: \n",
    "        return \"Positive\" \n",
    "    else: \n",
    "        return \"Negative\" \n",
    "\n",
    " # you can also use np.where() to get the same task done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:25:27.763225Z",
     "start_time": "2021-06-26T13:25:27.746269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the defined function on the column 'Avg' and creating a new column called 'Sentiment'\n",
    "Apple_tweets[\"Sentiment\"] = Apple_tweets[\"Avg\"].apply(get_senti)\n",
    "\n",
    "# Dropping the 'Avg' column from the data frame\n",
    "Apple_tweets.drop(\"Avg\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:26:28.974074Z",
     "start_time": "2021-06-26T13:26:28.954096Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the newly created 'Sentiment' variable has all the positive entries one after the other and all the negative entries after. Since we need to split the data into training and test randomly we have to jumble up the data set. We will use the DataFrame.sample() function.\n",
    "\n",
    "###### Note: We are not using the train-test split function from sklearn and hence the need to jumble the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:30:05.363497Z",
     "start_time": "2021-06-26T13:30:05.340558Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets=Apple_tweets.sample(frac=1,random_state=3).reset_index(drop=True)\n",
    "#pd.sample()=Return a random sample of items from an axis of object.\n",
    "#random_state:we used random_state for reproducibility.\n",
    "#frac=1: is used to generate random sample for whole of the dataset (without replacement)\n",
    "#Reset_index: To reset the index as it got shuffled.\n",
    "#Drop: We used it to drop the previous index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:30:59.041046Z",
     "start_time": "2021-06-26T13:30:59.020102Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect words from all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:33:11.527769Z",
     "start_time": "2021-06-26T13:33:11.514837Z"
    }
   },
   "outputs": [],
   "source": [
    "all_Words = (' '.join(Apple_tweets['Tweet'])).split() # tweets joined to eacch other using a space---> one long string with all the tweets connected. Then split this into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_Words = [x for x in (' '.join(Apple_tweets['Tweet']).split())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Frequency of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:35:35.840722Z",
     "start_time": "2021-06-26T13:35:35.802824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.FreqDist(all_Words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations & Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:37:03.577888Z",
     "start_time": "2021-06-26T13:37:03.561932Z"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:40:01.785821Z",
     "start_time": "2021-06-26T13:40:01.680100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a variable 'stopwords' which contains the list of punctuations from the string library and the english stopwords\n",
    "# from nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english') +list(string.punctuation)\n",
    "\n",
    "# Only keeping the words which are not the 'stopwords'\n",
    "all_words_clean = [word for word in all_Words if word not in stopwords]\n",
    "\n",
    "\n",
    "# Creating a frequency distribution of the lower case words which does not contain any stopwords\n",
    "all_words_freq = nltk.FreqDist(all_words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Frequency of Cleaned List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:40:06.643341Z",
     "start_time": "2021-06-26T13:40:06.624394Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:40:34.527879Z",
     "start_time": "2021-06-26T13:40:34.516871Z"
    }
   },
   "outputs": [],
   "source": [
    "len(all_words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take top 2000 words as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:43:13.581787Z",
     "start_time": "2021-06-26T13:43:13.566827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the  most common 2000 words after the list of words have been converted to lowercase and the stopwords have been removed\n",
    "word_features = [item[0] for item in all_words_freq.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:43:22.183566Z",
     "start_time": "2021-06-26T13:43:22.140678Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Check presence of each Feature in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:50:09.107511Z",
     "start_time": "2021-06-26T13:50:09.096542Z"
    }
   },
   "outputs": [],
   "source": [
    "## We are defining a function to appropriately process the text document\n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document) #getting the unique number of entries in the document variable\n",
    "    features = {} #defining an empty dictionary\n",
    "    for word in word_features: #looping over the 'word_features' which has been defined in the last code block\n",
    "        features[f'contains({word})'] = (word in document_words) #defining 'features' in  particular format and checking whether the unique elements of the input 'document' are contained in the 'word_features' defined before\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:50:11.689492Z",
     "start_time": "2021-06-26T13:50:11.636634Z"
    }
   },
   "outputs": [],
   "source": [
    "document_features(['apple','iphone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the list of words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:55:04.233302Z",
     "start_time": "2021-06-26T13:55:03.813357Z"
    }
   },
   "outputs": [],
   "source": [
    "frame = Apple_tweets.copy() #storing Apple_tweets in another variable\n",
    "frame.columns = [\"feature\", \"label\"] # defning the names of the colummn of the data frame 'frame'\n",
    "\n",
    "frame['feature'] = frame.apply(lambda x: nltk.word_tokenize(str(x['feature'])), axis=1) #the features of the 'frame' data frame are stored in the variable 'feature'\n",
    "# In the above code snippet we are tokenizing the variables\n",
    "frame['label'] = frame.label # the labels of the 'frame' data frame are stored in the variable 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:55:11.155467Z",
     "start_time": "2021-06-26T13:55:11.129537Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare the contents of Apple_tweets df and frame df\n",
    "Apple_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:55:21.052454Z",
     "start_time": "2021-06-26T13:55:21.022534Z"
    }
   },
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T13:55:52.777327Z",
     "start_time": "2021-06-26T13:55:52.755387Z"
    }
   },
   "outputs": [],
   "source": [
    "frame['feature'][88]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Feature set for each tweet\n",
    "\n",
    "Presence or absence of each of the 2000 words in the tweet along with the target variable value for each tweet.. this info is generated for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:01:59.033439Z",
     "start_time": "2021-06-26T14:01:57.433387Z"
    }
   },
   "outputs": [],
   "source": [
    "## We are now creating our combined data frame which we will split into training and test before fitting a classifier\n",
    "\n",
    "# We are creating a list the elements of which are a tuple. We are appending the list with tuples whose entries are the pre-processed tweets and the corresponding sentiment attached to it.\n",
    "featuresets = [(document_features(feature), label) for index, (feature, label) in frame.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:02:46.189745Z",
     "start_time": "2021-06-26T14:02:46.128877Z"
    }
   },
   "outputs": [],
   "source": [
    "featuresets[0] #feature values for the 1st tweet shown as sample. This is a tuple containing 2 elements. First element is a dictionary containing the 2000 words as the keys and values are true/false depending on if the word is found in the tweet. The second element of the tweet is the target variable value (positive / negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a nltk.Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:07:21.139397Z",
     "start_time": "2021-06-26T14:07:17.884979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "\n",
    "# first 70% of tweets taken in training set and remaining in test set\n",
    "# remember we have already randomly mixed up the tweets with respective labels\n",
    "\n",
    "train_set, test_set = featuresets[0:int(len(featuresets)*0.7)], featuresets[int(len(featuresets)*0.7):]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Predictions Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:09:54.004042Z",
     "start_time": "2021-06-26T14:09:50.924803Z"
    }
   },
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate Way: using Naive Bayes Classifier.\n",
    "Now, let us reload the data and look at other text mining functionalities that Python offers us and then go on to fit a classifier algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:12:06.984648Z",
     "start_time": "2021-06-26T14:12:06.958721Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Apple_tweets = pd.read_csv(\"Apple_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploration in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a temporary function lambda can be used. These functions do not require a name like a def function, however the output is same as defining a permanent function**\n",
    "**As these function are temporary, memory comsumption is less in comparison to permanent function. Also there are multiple ways to get a similar output**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:14:00.878785Z",
     "start_time": "2021-06-26T14:14:00.856845Z"
    }
   },
   "outputs": [],
   "source": [
    "## Let's get a word count without writing a lambda function\n",
    "\n",
    "# total words in each tweet created as a new column in the dataframe\n",
    "Apple_tweets['totalwords'] = [len(x.split()) for x in Apple_tweets['Tweet'].tolist()]\n",
    "\n",
    "\n",
    "Apple_tweets[['Tweet','totalwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:14:34.357361Z",
     "start_time": "2021-06-26T14:14:34.324452Z"
    }
   },
   "outputs": [],
   "source": [
    "# alternate way of doing the same thing\n",
    "Apple_tweets['word_count'] = Apple_tweets['Tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "Apple_tweets[['Tweet','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Characters- including spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:15:18.592260Z",
     "start_time": "2021-06-26T14:15:18.566329Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['char_count'] = Apple_tweets['Tweet'].str.len()\n",
    "\n",
    "Apple_tweets[['Tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:15:39.363884Z",
     "start_time": "2021-06-26T14:15:39.341945Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:16:14.873082Z",
     "start_time": "2021-06-26T14:16:14.856127Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Apple_tweets['Tweet'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:17:48.973726Z",
     "start_time": "2021-06-26T14:17:48.955774Z"
    }
   },
   "outputs": [],
   "source": [
    "# status = 'balaji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:18:07.225425Z",
     "start_time": "2021-06-26T14:18:07.219441Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in status:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:19:53.928769Z",
     "start_time": "2021-06-26T14:19:53.895858Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    #splitting the words separately from the input taken\n",
    "    words = sentence.split() \n",
    "    return (sum(len(word) for word in words)/len(words)) \n",
    "    # getting the average number of words in the each of the entries\n",
    "\n",
    "Apple_tweets['avg_word'] = Apple_tweets['Tweet'].apply(lambda x: avg_word(x))\n",
    "Apple_tweets[['Tweet','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:23:30.620561Z",
     "start_time": "2021-06-26T14:23:30.531796Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Apple_tweets['stopwords'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "Apple_tweets[['Tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate way - Nos of stop words\n",
    "nos_stop = [] # empty list to store count of stop words in each tweet\n",
    "for i in range(len(Apple_tweets)):\n",
    "    wrds = Apple_tweets['Tweet'][i].split()\n",
    "    nos_stop.append(len([words for words in wrds if words in stop]))\n",
    "\n",
    "nos_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:25:23.533879Z",
     "start_time": "2021-06-26T14:25:23.497974Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['hastags'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "Apple_tweets[['Tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:26:46.286322Z",
     "start_time": "2021-06-26T14:26:46.269370Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['numerics'] = Apple_tweets['Tweet'].apply(lambda x: len(re.findall(r'[0-9]',x)))\n",
    "Apple_tweets[['Tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:26:53.058454Z",
     "start_time": "2021-06-26T14:26:53.027537Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['upper'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "Apple_tweets[['Tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:27:22.088699Z",
     "start_time": "2021-06-26T14:27:22.065760Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['upper_letter'] = Apple_tweets['Tweet'].apply(lambda x: len(re.findall(r'[A-Z]',x)))\n",
    "Apple_tweets[['Tweet','upper_letter']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:28:42.170418Z",
     "start_time": "2021-06-26T14:28:42.136511Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of all non-alphanumric and non-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:28:54.561643Z",
     "start_time": "2021-06-26T14:28:54.544690Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "#\\w: Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "#\\s: Returns a match where the string contains a white space character.\n",
    "#[^]: Returns a match for any character EXCEPT what is written after it.\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:29:27.639950Z",
     "start_time": "2021-06-26T14:29:27.522265Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words Removal\n",
    "1. **We will create a list of 10 frequently occuring words and then decide if we need to remove it or retain it.**\n",
    "2. **Reason is that this file has tweets related to Apple.. So no point in keeping the word like Apple, unless we have tweets from other brands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:31:33.366807Z",
     "start_time": "2021-06-26T14:31:33.330903Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **As we are talking about multiple products hence iphone will be kept, similarly some tweets do relate to old products without mentioning the word old, hence even new would be kept in the tweets.**\n",
    "2. **hence only apple and get would be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:33:15.614745Z",
     "start_time": "2021-06-26T14:33:15.604775Z"
    }
   },
   "outputs": [],
   "source": [
    "freq =['apple','get']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:33:37.668276Z",
     "start_time": "2021-06-26T14:33:37.643343Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal\n",
    "**This is done as association of these less occurring words with the existing words could be a noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:34:43.767663Z",
     "start_time": "2021-06-26T14:34:43.741732Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts().tail(10)\n",
    "freq\n",
    "## As it is difficult to make out if these words will have association in text analytics or not, hence to start with these words are kept in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    " Refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:37:42.937884Z",
     "start_time": "2021-06-26T14:37:42.484099Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #snowball stemmer\n",
    "st = PorterStemmer()\n",
    "Apple_tweets['Tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T10:52:41.105760Z",
     "start_time": "2020-04-10T10:52:41.100002Z"
    }
   },
   "source": [
    "### Target Variable Conversion\n",
    "\n",
    "Now to get the sentiments as positive and negative , convert the Avg column . If value is >= 0  then tweet is Positive, else tweet is Negative. This will make a dependent variable as a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:40:04.300315Z",
     "start_time": "2021-06-26T14:40:04.286810Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets[\"Sentiment\"] = Apple_tweets[\"Avg\"].apply(get_senti) # get_senti is a user defined function we create earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:40:08.052964Z",
     "start_time": "2021-06-26T14:40:08.031023Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:40:48.407318Z",
     "start_time": "2021-06-26T14:40:48.380392Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Target Variable : Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:41:11.218014Z",
     "start_time": "2021-06-26T14:41:11.183107Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets.Sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:43:06.698059Z",
     "start_time": "2021-06-26T14:43:06.680104Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_features = Apple_tweets.iloc[:, 0].values # X_train..\n",
    "labels = Apple_tweets.iloc[:, 11].values # y_train.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:43:12.512293Z",
     "start_time": "2021-06-26T14:43:12.494358Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:43:18.173178Z",
     "start_time": "2021-06-26T14:43:18.158217Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "More here - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:48:56.831630Z",
     "start_time": "2021-06-26T14:48:56.819666Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:52:15.995936Z",
     "start_time": "2021-06-26T14:52:15.940085Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
    "\n",
    "# fit_transform() here returns a document term matrix\n",
    "# we are converting the output into an array so that we can put it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:54:05.702185Z",
     "start_time": "2021-06-26T14:54:05.638394Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Extra Knowledge Bytes (TF-IDF)\n",
    "\n",
    "# Let's see how our TD-IDF looks like (sorting by the feature named 5s)\n",
    "# Creating the TF-IDF with the feature names given by the TFIDF vectorizer, sorting it for unerstanding.\n",
    "# Let's chain the .head() method on the DataFrame to inspect the first few observations of the TD-IDF sorted by '5s'\n",
    "\n",
    "pd.DataFrame(processed_features, columns = vectorizer.get_feature_names()).sort_values(by = '5s', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:57:22.698017Z",
     "start_time": "2021-06-26T14:57:22.678034Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:57:25.990615Z",
     "start_time": "2021-06-26T14:57:25.975659Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[:10] # sample of first 10 values in target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:57:40.900124Z",
     "start_time": "2021-06-26T14:57:40.887159Z"
    }
   },
   "outputs": [],
   "source": [
    "# To model the Gaussian Navie Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:57:46.856604Z",
     "start_time": "2021-06-26T14:57:46.826687Z"
    }
   },
   "outputs": [],
   "source": [
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:57:54.463565Z",
     "start_time": "2021-06-26T14:57:54.448608Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_predict = NB_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:58:00.259094Z",
     "start_time": "2021-06-26T14:58:00.235159Z"
    }
   },
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "model_score = NB_model.score(X_train, y_train)                      \n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:58:26.474118Z",
     "start_time": "2021-06-26T14:58:26.278640Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:59:31.622128Z",
     "start_time": "2021-06-26T14:59:31.238900Z"
    }
   },
   "outputs": [],
   "source": [
    "## confusion_matrix\n",
    "plot_confusion_matrix(NB_model,X_train,y_train,colorbar=False);\n",
    "plt.grid(b=False, axis='both');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:59:36.659837Z",
     "start_time": "2021-06-26T14:59:36.607976Z"
    }
   },
   "outputs": [],
   "source": [
    "## classification_report\n",
    "print(classification_report(y_train,y_train_predict))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:59:47.468445Z",
     "start_time": "2021-06-26T14:59:47.456478Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performance Matrix on test data set\n",
    "y_test_predict = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:59:48.897971Z",
     "start_time": "2021-06-26T14:59:48.881985Z"
    }
   },
   "outputs": [],
   "source": [
    "model_score = NB_model.score(X_test, y_test) # accuracy\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T14:59:54.545631Z",
     "start_time": "2021-06-26T14:59:54.312222Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(NB_model,X_test, y_test,colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T15:00:02.553643Z",
     "start_time": "2021-06-26T15:00:02.525720Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pl. note - Model building is an iterative process. Model performance both on the test and train dataset can be improved using feature engineering, feature extraction, hyper parameter tuning (including combination of various parameters).** \n",
    "\n",
    "**Model has to match the business objective and hence various permutation and combinations can be tried on to refine the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up a bit more !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T15:08:19.497353Z",
     "start_time": "2021-06-26T15:08:19.476374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing symbols and punctuations \n",
    "# further_clean = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Extending the list of stop words (including words like Apple, bitly, dear, please, etc.)\n",
    "stop_words = list(stopwords.words('english'))\n",
    "stop_words.extend([\"apple\", \"http\",\"bit\",\"bitly\",\"bit ly\", \"dear\", \"im\", \"i'm\", \"please\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T15:08:29.334493Z",
     "start_time": "2021-06-26T15:08:29.268637Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing stop words (extended list as above) from the corpus \n",
    "\n",
    "corpus = Apple_tweets['Tweet'].apply(lambda x: ' '.join([z for z in x.split() if z not in stop_words])) \n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T15:09:04.224408Z",
     "start_time": "2021-06-26T15:09:04.207453Z"
    }
   },
   "outputs": [],
   "source": [
    "wc_a = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T15:11:39.108465Z",
     "start_time": "2021-06-26T15:11:07.857426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word Cloud \n",
    "from wordcloud import WordCloud\n",
    "# wordcloud = WordCloud().generate(wc_a) if ok with default wordcloud parameters\n",
    "\n",
    "wordcloud = WordCloud(width = 3000, height = 3000, \n",
    "                background_color ='black', \n",
    "                min_font_size = 10, random_state=100).generate(wc_a) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.xlabel('Word Cloud')\n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "print(\"Word Cloud for Apple_Tweets (after cleaning)!!\")\n",
    "\n",
    "\n",
    "#Tip: You can specify stopwords, regex (punctuations/symbols) in the wordcloud itself, check CTRL+TAB on the wordcloud function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
